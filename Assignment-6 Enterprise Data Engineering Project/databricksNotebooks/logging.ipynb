{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e21ee43-3570-4de5-be90-badb70b2b01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define logging schema explicitly\n",
    "log_schema = StructType([\n",
    "    StructField(\"pipeline_layer\", StringType(), True),\n",
    "    StructField(\"records_processed\", IntegerType(), True),\n",
    "    StructField(\"records_rejected\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"start_ts\", TimestampType(), True),\n",
    "    StructField(\"end_ts\", TimestampType(), True),\n",
    "    StructField(\"remarks\", StringType(), True)\n",
    "])\n",
    "\n",
    "def log_pipeline(layer, processed, rejected, status, remarks=\"\"):\n",
    "    now = datetime.now()\n",
    "    log_df = spark.createDataFrame([\n",
    "        (layer, processed, rejected, status, now, now, remarks)\n",
    "    ], schema=log_schema)\n",
    "    \n",
    "    log_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"workspace.default.pipeline_logs\")\n",
    "\n",
    "# Bronze Layer Logging (optional, counts existing Delta table if present)\n",
    "try:\n",
    "    log_pipeline(\"Bronze Layer\", 0, 0, \"STARTED\")\n",
    "    \n",
    "    bronze_exists = spark.catalog.tableExists(\"workspace.default.bronze_sales_transactions\")\n",
    "    if bronze_exists:\n",
    "        bronze_sales_df = spark.read.table(\"workspace.default.bronze_sales_transactions\")\n",
    "        bronze_count = bronze_sales_df.count()\n",
    "    else:\n",
    "        bronze_count = 0\n",
    "\n",
    "    log_pipeline(\"Bronze Layer\", bronze_count, 0, \"COMPLETED\")\n",
    "except Exception as e:\n",
    "    log_pipeline(\"Bronze Layer\", 0, 0, \"FAILED\", str(e))\n",
    "    raise e\n",
    "\n",
    "# Silver Layer Logging\n",
    "try:\n",
    "    log_pipeline(\"Silver Layer\", 0, 0, \"STARTED\")\n",
    "    \n",
    "    silver_sales_df = spark.read.table(\"workspace.default.silver_sales_transactions\")\n",
    "    quarantine_df = spark.read.table(\"workspace.default.silver_sales_quarantine\")\n",
    "    \n",
    "    valid_count = silver_sales_df.count()\n",
    "    rejected_count = quarantine_df.count()\n",
    "    \n",
    "    log_pipeline(\"Silver Layer\", valid_count, rejected_count, \"COMPLETED\", \"Invalid records moved to quarantine\")\n",
    "except Exception as e:\n",
    "    log_pipeline(\"Silver Layer\", 0, 0, \"FAILED\", str(e))\n",
    "    raise e\n",
    "\n",
    "# Gold Layer Logging\n",
    "try:\n",
    "    log_pipeline(\"Gold Layer\", 0, 0, \"STARTED\")\n",
    "    \n",
    "    gold_daily_df = spark.read.table(\"workspace.default.gold_daily_sales\")\n",
    "    gold_count = gold_daily_df.count()\n",
    "    \n",
    "    log_pipeline(\"Gold Layer\", gold_count, 0, \"COMPLETED\")\n",
    "except Exception as e:\n",
    "    log_pipeline(\"Gold Layer\", 0, 0, \"FAILED\", str(e))\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}